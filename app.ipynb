{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Parameters\n",
    "# detector = True\n",
    "# fcg_path_benign='/mnt/E/re/Embedding vector generation based on function call graph for effective malware detection and classification/FCGs/benign'\n",
    "# fcg_path_malware= '/mnt/E/re/Embedding vector generation based on function call graph for effective malware detection and classification/FCGs/malware'\n",
    "# w2v_path='./model_params/word2vec.wordvectors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "def count_pickle_files(directory):\n",
    "    count = 0\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.pickle'):\n",
    "                count += 1\n",
    "    return count\n",
    "\n",
    "def load_pickle(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        return pickle.load(file)\n",
    "\n",
    "def create_dataframe(directory, label):\n",
    "    data = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.pickle'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                file_data = load_pickle(file_path)\n",
    "                data.append({\n",
    "                    'filename': file,\n",
    "                    'label': label,\n",
    "                    'file_path': file_path\n",
    "                })\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Sample larger than population or is negative",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_801331/905398503.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_801331/905398503.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# Randomly sample 15,000 files from each directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0msample_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0msampled_benign_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_json_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbenign_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0msampled_malware_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_json_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmalware_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_801331/905398503.py\u001b[0m in \u001b[0;36msample_json_files\u001b[0;34m(json_files, sample_size)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msample_json_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprocess_json_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/random.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, population, k, counts)\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[0mrandbelow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_randbelow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sample larger than population or is negative\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0msetsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m21\u001b[0m        \u001b[0;31m# size of a small set minus size of an empty list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Sample larger than population or is negative"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from dask import delayed\n",
    "\n",
    "def list_json_files(directory):\n",
    "    json_files = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.json'):\n",
    "                json_files.append(os.path.join(root, file))\n",
    "    return json_files\n",
    "\n",
    "def sample_json_files(json_files, sample_size):\n",
    "    return random.sample(json_files, sample_size)\n",
    "\n",
    "def process_json_file(file_path, label):\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            json_data = json.load(f)\n",
    "            json_data['label'] = label\n",
    "            json_data['file_path'] = file_path\n",
    "            return json_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_json_files(file_paths, label):\n",
    "    data = []\n",
    "    for file_path in file_paths:\n",
    "        json_data = process_json_file(file_path, label)\n",
    "        if json_data:\n",
    "            data.append(json_data)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "@delayed\n",
    "def delayed_process_json_files(file_paths, label):\n",
    "    return process_json_files(file_paths, label)\n",
    "\n",
    "def main():\n",
    "    base_dir = '/mnt/E/mnt/bigDisk/yishan/dataset_disassemble/results'\n",
    "    benign_dir = os.path.join(base_dir, 'benign')\n",
    "    malware_dir = os.path.join(base_dir, 'malware')\n",
    "\n",
    "    benign_files = list_json_files(benign_dir)\n",
    "    malware_files = list_json_files(malware_dir)\n",
    "\n",
    "    # Randomly sample files from each directory\n",
    "    sample_size = 15000\n",
    "    sampled_benign_files = sample_json_files(benign_files, sample_size)\n",
    "    sampled_malware_files = sample_json_files(malware_files, sample_size)\n",
    "\n",
    "    # Use delayed processing for large datasets\n",
    "    benign_df_delayed = delayed_process_json_files(sampled_benign_files, 0)\n",
    "    malware_df_delayed = delayed_process_json_files(sampled_malware_files, 1)\n",
    "\n",
    "    # Combine and compute the final DataFrame\n",
    "    combined_df = dd.from_delayed([benign_df_delayed, malware_df_delayed])\n",
    "    \n",
    "    # Compute the combined DataFrame\n",
    "    final_df = combined_df.compute()\n",
    "    print(final_df)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- My understanding of Normalization is based on the goals mentioned in the paper, which include the following:\n",
    "1. Remove all comments: In radare2, only ; is used for comments. Refer to 7.1 Adding Metadata for more information.\n",
    "2. Replace all numeric constant values with \"N\": Use re.sub() to iterate through all numeric constants with the regular expression \\b0x[a-fA-F0-9]+\\b|\\b\\d+\\b.\n",
    "3. Replace all irregular strings with \"M\": Use re.sub() to iterate through all irregular strings with the regular expression \\b[^a-zA-Z0-9]+\\b.\n",
    "4. Replace all function names with their short names: Use re.sub() to iterate through all common function names with the regular expression \\b(sub|loc|str|reloc|obj)_[0-9a-fA-F]+\\b.\n",
    "5. Connect the opcode and operand with \"-\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class NormalizeAssembly:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def normalize(self, inst):\n",
    "        \"\"\"\n",
    "        Normalize a given assembly instruction. According to the paper should have:\n",
    "        1. Remove all comments;\n",
    "        2. Replace all numeric constant values with \"N\";\n",
    "        3. Replace all irregular strings with \"M\";\n",
    "        4. Replace all function names with their short name;\n",
    "           For example, replace \"sub_406492\" with \"sub\", replace \"loc_100080CF\" with \"loc\".\n",
    "        5. Connect the opcode and operand with \"-\"\n",
    "        \"\"\"\n",
    "        # Remove comments\n",
    "        inst = re.sub(r';.*', '', inst)\n",
    "\n",
    "        # Replace numeric constant values with \"N\"\n",
    "        inst = re.sub(r'\\b0x[0-9a-fA-F]+\\b|\\b\\d+\\b', 'N', inst)\n",
    "\n",
    "        # Replace function names with their short name\n",
    "        inst = re.sub(r'\\b(sub|loc|str|reloc|obj)_[0-9a-fA-F]+\\b', r'\\1', inst)\n",
    "\n",
    "        # Split the instruction into parts to process irregular strings\n",
    "        asm_normed = []\n",
    "        parts = re.split(r'(\\s+)', inst)\n",
    "\n",
    "        for part in parts:\n",
    "            if re.match(r'\\b[^a-zA-Z0-9]+\\b', part):\n",
    "                part = 'M'\n",
    "            asm_normed.append(part)\n",
    "\n",
    "        # Join the parts back together\n",
    "        normalized_inst = ''.join(asm_normed)\n",
    "\n",
    "        # Connect opcode and operand with \"-\"\n",
    "        inst_parts = normalized_inst.split()\n",
    "        if len(inst_parts) > 1:\n",
    "            normalized_inst = f\"{inst_parts[0]}-{' '.join(inst_parts[1:])}\"\n",
    "        else:\n",
    "            normalized_inst = inst_parts[0]\n",
    "\n",
    "        return normalized_inst\n",
    "\n",
    "def normalize_dataframe(df, normalizer):\n",
    "    df['data'] = df['data'].apply(lambda instructions: [normalizer.normalize(inst) for inst in instructions] if isinstance(instructions, list) else instructions)\n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    normalizer = NormalizeAssembly()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My understanding is that CBOW is used for predicting a word from its context, while Skip-gram does the opposite. Thus, the inputs and outputs are reversed for these two models. In the end, a softmax function is used to predict a probability distribution.\n",
    "\n",
    "- vector_size: The dimensionality of the word vectors. The default value is 100. Generally, a larger vector size is better but requires more computational resources.\n",
    "- window: The size of the context window, which determines the number of words to consider around the target word. The default value is 5.\n",
    "- alpha: The learning rate. The default value is 0.025. The learning rate gradually decreases during training.\n",
    "- min_count: Ignores all words with a frequency lower than this value. The default value is 5.\n",
    "- sg: The training algorithm choice. 1 means Skip-gram is used, while 0 means CBOW is used. The default value is 0.\n",
    "- hs: Indicates whether hierarchical softmax is used. The default value is 0, meaning it is not used.\n",
    "- negative: The number of negative samples to use when negative sampling is employed. The default value is 5.\n",
    "- epochs: The number of training iterations. The default value is 5\n",
    "\n",
    "I spent some time on hyperparameter adjustment. After reading this paper, I chose to use Xavier initialization for the model rather than importing the model from Gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "class Word2VecCBOW:\n",
    "    def __init__(self, vocab_size, hidden_dim):\n",
    "        # Initialize the vocabulary size and hidden layer dimension\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Initialize weights with Xavier initialization\n",
    "        # W1 is the weight matrix from input layer to hidden layer\n",
    "        self.W1 = np.random.randn(\n",
    "            vocab_size, hidden_dim) / np.sqrt(vocab_size / 2)\n",
    "        # W2 is the weight matrix from hidden layer to output layer\n",
    "        self.W2 = np.random.randn(\n",
    "            hidden_dim, vocab_size) / np.sqrt(hidden_dim / 2)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        # Compute the softmax of vector x in a numerically stable way\n",
    "        exp_x = np.exp(x - np.max(x))\n",
    "        return exp_x / np.sum(exp_x, axis=0)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Forward pass: compute hidden layer activations and output layer\n",
    "        # X is the one-hot encoded input vector\n",
    "        self.h = np.dot(X, self.W1)  # Hidden layer activations\n",
    "        self.u = np.dot(self.h, self.W2)  # Output layer scores\n",
    "        self.y_hat = self.softmax(self.u)  # Output probabilities\n",
    "        return self.y_hat\n",
    "\n",
    "    def backward(self, X, y, learning_rate):\n",
    "        # Backward pass: compute gradients and update weights\n",
    "        # y_hat is the predicted output from the forward pass\n",
    "        # y is the true one-hot encoded target vector\n",
    "        error = self.y_hat - y  # Compute error\n",
    "\n",
    "        # Compute gradients for W2 and W1\n",
    "        dW2 = np.dot(self.h.T, error)  # Gradient for W2\n",
    "        dW1 = np.dot(X.T, np.dot(error, self.W2.T))  # Gradient for W1\n",
    "\n",
    "        # Update the weights using gradient descent\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.W1 -= learning_rate * dW1\n",
    "\n",
    "    def train(self, sentences, learning_rate=0.01, epochs=10):\n",
    "        # Build vocabulary and initialize one-hot encoder\n",
    "        word2index = {word: i for i, word in enumerate(\n",
    "            set(sum(sentences, [])))}\n",
    "        one_hot_encoder = OneHotEncoder(categories=[list(word2index.values())])\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            for sentence in sentences:\n",
    "                for i, word in enumerate(sentence):\n",
    "                    # Define the context window (size 2)\n",
    "                    context = [word2index[w] for w in sentence[max(\n",
    "                        0, i - 2):i] + sentence[i + 1:i + 3]]\n",
    "                    target = word2index[word]\n",
    "\n",
    "                    # One-hot encode the context and target\n",
    "                    X = one_hot_encoder.fit_transform([[context]]).toarray()\n",
    "                    y = one_hot_encoder.fit_transform(\n",
    "                        [[target]]).toarray().flatten()\n",
    "\n",
    "                    # Forward and backward pass\n",
    "                    self.forward(X)\n",
    "                    self.backward(X, y, learning_rate)\n",
    "\n",
    "    def get_word_vectors(self):\n",
    "        # Return the learned word vectors\n",
    "        return self.W1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paper Description: \n",
    "1. Randomly initialize embedding vectors.\n",
    "2. Each vertex's embedding vector ùë¢ùëñ is updated in each round by combining its own features ùë•ùë£ùëñ and the embedding vectors of its neighboring vertices ùë¢ùëó.\n",
    "3. Assign weights ùëéùëñ based on the importance of each vertex.\n",
    "4. Combine the vertex embedding vectors ùë¢ùëñ using the attention weights ùëéùëñ to obtain the final graph embedding vector ùëî.\n",
    "5. Use a two-layer feed-forward neural network for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool as gmp\n",
    "\n",
    "\n",
    "class GCNWithAttention(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels, num_classes):\n",
    "        super(GCNWithAttention, self).__init__()\n",
    "        # Initialize the first GCN layer\n",
    "        self.conv1 = GCNConv(num_features, hidden_channels)\n",
    "        # Initialize the second GCN layer\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        # Initialize the first linear layer\n",
    "        self.lin1 = Linear(hidden_channels, 128)\n",
    "        # Initialize the second linear layer\n",
    "        self.lin2 = Linear(128, 128)\n",
    "        # Initialize the attention layer\n",
    "        self.attention = Linear(hidden_channels, 1)\n",
    "        # Initialize the final linear layer for classification\n",
    "        self.lin = Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, data_batch):\n",
    "        # Extract node features, edge indices, and batch information from the data batch\n",
    "        x, edge_index, batch = data_batch.x, data_batch.edge_index, data_batch.batch\n",
    "\n",
    "        # Perform the first GCN layer transformation\n",
    "        x = self.conv1(x, edge_index)\n",
    "        # Apply a ReLU non-linear activation function\n",
    "        x = x.relu()\n",
    "        # Perform the second GCN layer transformation\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        # Apply the attention mechanism\n",
    "        attn_weights = F.leaky_relu(self.attention(x))\n",
    "        # Apply softmax to normalize the attention weights\n",
    "        attn_weights = F.softmax(attn_weights, dim=0)\n",
    "        # Multiply the node features by the attention weights\n",
    "        x = x * attn_weights\n",
    "\n",
    "        # Apply global mean pooling to get graph-level representation\n",
    "        x = gmp(x, batch)\n",
    "\n",
    "        # Pass the pooled representation through the first linear layer\n",
    "        x = self.lin1(x)\n",
    "        # Apply a ReLU non-linear activation function\n",
    "        x = F.relu(x)\n",
    "        # Pass through the second linear layer\n",
    "        x = self.lin2(x)\n",
    "        # Apply a ReLU non-linear activation function\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Apply dropout to prevent overfitting\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        # Pass through the final linear layer for classification\n",
    "        x = self.lin(x)\n",
    "\n",
    "        # Apply softmax to get output probabilities\n",
    "        x = F.softmax(x, dim=1)\n",
    "\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
