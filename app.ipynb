{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Parameters\n",
    "# detector = True\n",
    "# fcg_path_benign='/mnt/E/re/Embedding vector generation based on function call graph for effective malware detection and classification/FCGs/benign'\n",
    "# fcg_path_malware= '/mnt/E/re/Embedding vector generation based on function call graph for effective malware detection and classification/FCGs/malware'\n",
    "# w2v_path='./model_params/word2vec.wordvectors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "def count_pickle_files(directory):\n",
    "    count = 0\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.pickle'):\n",
    "                count += 1\n",
    "    return count\n",
    "\n",
    "def load_pickle(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        return pickle.load(file)\n",
    "\n",
    "def create_dataframe(directory, label):\n",
    "    data = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.pickle'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                file_data = load_pickle(file_path)\n",
    "                data.append({\n",
    "                    'filename': file,\n",
    "                    'label': label,\n",
    "                    'file_path': file_path\n",
    "                })\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Sample larger than population or is negative",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_801331/905398503.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_801331/905398503.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# Randomly sample 15,000 files from each directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0msample_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0msampled_benign_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_json_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbenign_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0msampled_malware_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_json_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmalware_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_801331/905398503.py\u001b[0m in \u001b[0;36msample_json_files\u001b[0;34m(json_files, sample_size)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msample_json_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprocess_json_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/random.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, population, k, counts)\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[0mrandbelow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_randbelow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sample larger than population or is negative\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0msetsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m21\u001b[0m        \u001b[0;31m# size of a small set minus size of an empty list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Sample larger than population or is negative"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from dask import delayed\n",
    "\n",
    "def list_json_files(directory):\n",
    "    json_files = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.json'):\n",
    "                json_files.append(os.path.join(root, file))\n",
    "    return json_files\n",
    "\n",
    "def sample_json_files(json_files, sample_size):\n",
    "    return random.sample(json_files, sample_size)\n",
    "\n",
    "def process_json_file(file_path, label):\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            json_data = json.load(f)\n",
    "            json_data['label'] = label\n",
    "            json_data['file_path'] = file_path\n",
    "            return json_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_json_files(file_paths, label):\n",
    "    data = []\n",
    "    for file_path in file_paths:\n",
    "        json_data = process_json_file(file_path, label)\n",
    "        if json_data:\n",
    "            data.append(json_data)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "@delayed\n",
    "def delayed_process_json_files(file_paths, label):\n",
    "    return process_json_files(file_paths, label)\n",
    "\n",
    "def main():\n",
    "    base_dir = '/mnt/E/mnt/bigDisk/yishan/dataset_disassemble/results'\n",
    "    benign_dir = os.path.join(base_dir, 'benign')\n",
    "    malware_dir = os.path.join(base_dir, 'malware')\n",
    "\n",
    "    benign_files = list_json_files(benign_dir)\n",
    "    malware_files = list_json_files(malware_dir)\n",
    "\n",
    "    # Randomly sample files from each directory\n",
    "    sample_size = 15000\n",
    "    sampled_benign_files = sample_json_files(benign_files, sample_size)\n",
    "    sampled_malware_files = sample_json_files(malware_files, sample_size)\n",
    "\n",
    "    # Use delayed processing for large datasets\n",
    "    benign_df_delayed = delayed_process_json_files(sampled_benign_files, 0)\n",
    "    malware_df_delayed = delayed_process_json_files(sampled_malware_files, 1)\n",
    "\n",
    "    # Combine and compute the final DataFrame\n",
    "    combined_df = dd.from_delayed([benign_df_delayed, malware_df_delayed])\n",
    "    \n",
    "    # Compute the combined DataFrame\n",
    "    final_df = combined_df.compute()\n",
    "    print(final_df)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- My understanding of Normalization is based on the goals mentioned in the paper, which include the following:\n",
    "1. Remove all comments: In radare2, only ; is used for comments. Refer to 7.1 Adding Metadata for more information.\n",
    "2. Replace all numeric constant values with \"N\": Use re.sub() to iterate through all numeric constants with the regular expression \\b0x[a-fA-F0-9]+\\b|\\b\\d+\\b.\n",
    "3. Replace all irregular strings with \"M\": Use re.sub() to iterate through all irregular strings with the regular expression \\b[^a-zA-Z0-9]+\\b.\n",
    "4. Replace all function names with their short names: Use re.sub() to iterate through all common function names with the regular expression \\b(sub|loc|str|reloc|obj)_[0-9a-fA-F]+\\b.\n",
    "5. Connect the opcode and operand with \"-\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class NormalizeAssembly:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def normalize(self, inst):\n",
    "        \"\"\"\n",
    "        Normalize a given assembly instruction. According to the paper should have:\n",
    "        1. Remove all comments;\n",
    "        2. Replace all numeric constant values with \"N\";\n",
    "        3. Replace all irregular strings with \"M\";\n",
    "        4. Replace all function names with their short name;\n",
    "           For example, replace \"sub_406492\" with \"sub\", replace \"loc_100080CF\" with \"loc\".\n",
    "        5. Connect the opcode and operand with \"-\"\n",
    "        \"\"\"\n",
    "        # Remove comments\n",
    "        inst = re.sub(r';.*', '', inst)\n",
    "\n",
    "        # Replace numeric constant values with \"N\"\n",
    "        inst = re.sub(r'\\b0x[0-9a-fA-F]+\\b|\\b\\d+\\b', 'N', inst)\n",
    "\n",
    "        # Replace function names with their short name\n",
    "        inst = re.sub(r'\\b(sub|loc|str|reloc|obj)_[0-9a-fA-F]+\\b', r'\\1', inst)\n",
    "\n",
    "        # Split the instruction into parts to process irregular strings\n",
    "        asm_normed = []\n",
    "        parts = re.split(r'(\\s+)', inst)\n",
    "\n",
    "        for part in parts:\n",
    "            if re.match(r'\\b[^a-zA-Z0-9]+\\b', part):\n",
    "                part = 'M'\n",
    "            asm_normed.append(part)\n",
    "\n",
    "        # Join the parts back together\n",
    "        normalized_inst = ''.join(asm_normed)\n",
    "\n",
    "        # Connect opcode and operand with \"-\"\n",
    "        inst_parts = normalized_inst.split()\n",
    "        if len(inst_parts) > 1:\n",
    "            normalized_inst = f\"{inst_parts[0]}-{' '.join(inst_parts[1:])}\"\n",
    "        else:\n",
    "            normalized_inst = inst_parts[0]\n",
    "\n",
    "        return normalized_inst\n",
    "\n",
    "def normalize_dataframe(df, normalizer):\n",
    "    df['data'] = df['data'].apply(lambda instructions: [normalizer.normalize(inst) for inst in instructions] if isinstance(instructions, list) else instructions)\n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    normalizer = NormalizeAssembly()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My understanding is that CBOW is used for predicting a word from its context, while Skip-gram does the opposite. Thus, the inputs and outputs are reversed for these two models. In the end, a softmax function is used to predict a probability distribution.\n",
    "\n",
    "- vector_size: The dimensionality of the word vectors. The default value is 100. Generally, a larger vector size is better but requires more computational resources.\n",
    "- window: The size of the context window, which determines the number of words to consider around the target word. The default value is 5.\n",
    "- alpha: The learning rate. The default value is 0.025. The learning rate gradually decreases during training.\n",
    "- min_count: Ignores all words with a frequency lower than this value. The default value is 5.\n",
    "- sg: The training algorithm choice. 1 means Skip-gram is used, while 0 means CBOW is used. The default value is 0.\n",
    "- hs: Indicates whether hierarchical softmax is used. The default value is 0, meaning it is not used.\n",
    "- negative: The number of negative samples to use when negative sampling is employed. The default value is 5.\n",
    "- epochs: The number of training iterations. The default value is 5\n",
    "\n",
    "I spent some time on hyperparameter adjustment. After reading this paper, I chose to use Xavier initialization for the model rather than importing the model from Gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "class Word2VecCBOW:\n",
    "    def __init__(self, vocab_size, hidden_dim):\n",
    "        # Initialize the vocabulary size and hidden layer dimension\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Initialize weights with Xavier initialization\n",
    "        # W1 is the weight matrix from input layer to hidden layer\n",
    "        self.W1 = np.random.randn(\n",
    "            vocab_size, hidden_dim) / np.sqrt(vocab_size / 2)\n",
    "        # W2 is the weight matrix from hidden layer to output layer\n",
    "        self.W2 = np.random.randn(\n",
    "            hidden_dim, vocab_size) / np.sqrt(hidden_dim / 2)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        # Compute the softmax of vector x in a numerically stable way\n",
    "        exp_x = np.exp(x - np.max(x))\n",
    "        return exp_x / np.sum(exp_x, axis=0)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Forward pass: compute hidden layer activations and output layer\n",
    "        # X is the one-hot encoded input vector\n",
    "        self.h = np.dot(X, self.W1)  # Hidden layer activations\n",
    "        self.u = np.dot(self.h, self.W2)  # Output layer scores\n",
    "        self.y_hat = self.softmax(self.u)  # Output probabilities\n",
    "        return self.y_hat\n",
    "\n",
    "    def backward(self, X, y, learning_rate):\n",
    "        # Backward pass: compute gradients and update weights\n",
    "        # y_hat is the predicted output from the forward pass\n",
    "        # y is the true one-hot encoded target vector\n",
    "        error = self.y_hat - y  # Compute error\n",
    "\n",
    "        # Compute gradients for W2 and W1\n",
    "        dW2 = np.dot(self.h.T, error)  # Gradient for W2\n",
    "        dW1 = np.dot(X.T, np.dot(error, self.W2.T))  # Gradient for W1\n",
    "\n",
    "        # Update the weights using gradient descent\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.W1 -= learning_rate * dW1\n",
    "\n",
    "    def train(self, sentences, learning_rate=0.01, epochs=10):\n",
    "        # Build vocabulary and initialize one-hot encoder\n",
    "        word2index = {word: i for i, word in enumerate(\n",
    "            set(sum(sentences, [])))}\n",
    "        one_hot_encoder = OneHotEncoder(categories=[list(word2index.values())])\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            for sentence in sentences:\n",
    "                for i, word in enumerate(sentence):\n",
    "                    # Define the context window (size 2)\n",
    "                    context = [word2index[w] for w in sentence[max(\n",
    "                        0, i - 2):i] + sentence[i + 1:i + 3]]\n",
    "                    target = word2index[word]\n",
    "\n",
    "                    # One-hot encode the context and target\n",
    "                    X = one_hot_encoder.fit_transform([[context]]).toarray()\n",
    "                    y = one_hot_encoder.fit_transform(\n",
    "                        [[target]]).toarray().flatten()\n",
    "\n",
    "                    # Forward and backward pass\n",
    "                    self.forward(X)\n",
    "                    self.backward(X, y, learning_rate)\n",
    "\n",
    "    def get_word_vectors(self):\n",
    "        # Return the learned word vectors\n",
    "        return self.W1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paper Description: \n",
    "1. Randomly initialize embedding vectors.\n",
    "2. Each vertex's embedding vector 𝑢𝑖 is updated in each round by combining its own features 𝑥𝑣𝑖 and the embedding vectors of its neighboring vertices 𝑢𝑗.\n",
    "3. Assign weights 𝑎𝑖 based on the importance of each vertex.\n",
    "4. Combine the vertex embedding vectors 𝑢𝑖 using the attention weights 𝑎𝑖 to obtain the final graph embedding vector 𝑔.\n",
    "5. Use a two-layer feed-forward neural network for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool as gmp\n",
    "\n",
    "\n",
    "class GCNWithAttention(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels, num_classes):\n",
    "        super(GCNWithAttention, self).__init__()\n",
    "        # Initialize the first GCN layer\n",
    "        self.conv1 = GCNConv(num_features, hidden_channels)\n",
    "        # Initialize the second GCN layer\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        # Initialize the first linear layer\n",
    "        self.lin1 = Linear(hidden_channels, 128)\n",
    "        # Initialize the second linear layer\n",
    "        self.lin2 = Linear(128, 128)\n",
    "        # Initialize the attention layer\n",
    "        self.attention = Linear(hidden_channels, 1)\n",
    "        # Initialize the final linear layer for classification\n",
    "        self.lin = Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, data_batch):\n",
    "        # Extract node features, edge indices, and batch information from the data batch\n",
    "        x, edge_index, batch = data_batch.x, data_batch.edge_index, data_batch.batch\n",
    "\n",
    "        # Perform the first GCN layer transformation\n",
    "        x = self.conv1(x, edge_index)\n",
    "        # Apply a ReLU non-linear activation function\n",
    "        x = x.relu()\n",
    "        # Perform the second GCN layer transformation\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        # Apply the attention mechanism\n",
    "        attn_weights = F.leaky_relu(self.attention(x))\n",
    "        # Apply softmax to normalize the attention weights\n",
    "        attn_weights = F.softmax(attn_weights, dim=0)\n",
    "        # Multiply the node features by the attention weights\n",
    "        x = x * attn_weights\n",
    "\n",
    "        # Apply global mean pooling to get graph-level representation\n",
    "        x = gmp(x, batch)\n",
    "\n",
    "        # Pass the pooled representation through the first linear layer\n",
    "        x = self.lin1(x)\n",
    "        # Apply a ReLU non-linear activation function\n",
    "        x = F.relu(x)\n",
    "        # Pass through the second linear layer\n",
    "        x = self.lin2(x)\n",
    "        # Apply a ReLU non-linear activation function\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Apply dropout to prevent overfitting\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        # Pass through the final linear layer for classification\n",
    "        x = self.lin(x)\n",
    "\n",
    "        # Apply softmax to get output probabilities\n",
    "        x = F.softmax(x, dim=1)\n",
    "\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
