{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Parameters\n",
    "# detector = True\n",
    "# fcg_path_benign='/mnt/E/re/Embedding vector generation based on function call graph for effective malware detection and classification/FCGs/benign'\n",
    "# fcg_path_malware= '/mnt/E/re/Embedding vector generation based on function call graph for effective malware detection and classification/FCGs/malware'\n",
    "# w2v_path='./model_params/word2vec.wordvectors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "def count_pickle_files(directory):\n",
    "    count = 0\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.pickle'):\n",
    "                count += 1\n",
    "    return count\n",
    "\n",
    "def load_pickle(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        return pickle.load(file)\n",
    "\n",
    "def create_dataframe(directory, label):\n",
    "    data = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.pickle'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                file_data = load_pickle(file_path)\n",
    "                data.append({\n",
    "                    'filename': file,\n",
    "                    'label': label,\n",
    "                    'file_path': file_path\n",
    "                })\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Sample larger than population or is negative",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_801331/905398503.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_801331/905398503.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# Randomly sample 15,000 files from each directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0msample_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0msampled_benign_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_json_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbenign_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0msampled_malware_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_json_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmalware_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_801331/905398503.py\u001b[0m in \u001b[0;36msample_json_files\u001b[0;34m(json_files, sample_size)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msample_json_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprocess_json_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/random.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, population, k, counts)\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[0mrandbelow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_randbelow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sample larger than population or is negative\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0msetsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m21\u001b[0m        \u001b[0;31m# size of a small set minus size of an empty list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Sample larger than population or is negative"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "\n",
    "def list_json_files(directory):\n",
    "    json_files = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.json'):\n",
    "                json_files.append(os.path.join(root, file))\n",
    "    return json_files\n",
    "\n",
    "def sample_json_files(json_files, sample_size):\n",
    "    return random.sample(json_files, sample_size)\n",
    "\n",
    "def process_json_files(file_paths, label):\n",
    "    data = []\n",
    "    for file_path in file_paths:\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                json_data = json.load(f)\n",
    "                json_data['label'] = label\n",
    "                json_data['file_path'] = file_path\n",
    "                data.append(json_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {e}\")\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def main():\n",
    "    base_dir = '/mnt/E/mnt/bigDisk/yishan/dataset_disassemble/results'\n",
    "    benign_dir = os.path.join(base_dir, 'benign')\n",
    "    malware_dir = os.path.join(base_dir, 'malware')\n",
    "\n",
    "    benign_files = list_json_files(benign_dir)\n",
    "    malware_files = list_json_files(malware_dir)\n",
    "\n",
    "    # Randomly sample 15,000 files from each directory\n",
    "    sample_size = 30000\n",
    "    sampled_benign_files = sample_json_files(benign_files, sample_size)\n",
    "    sampled_malware_files = sample_json_files(malware_files, sample_size)\n",
    "\n",
    "    # Process the sampled files\n",
    "    benign_df = process_json_files(sampled_benign_files, 0)\n",
    "    malware_df = process_json_files(sampled_malware_files, 1)\n",
    "\n",
    "    combined_df = dd.from_pandas(pd.concat([benign_df, malware_df], ignore_index=True), npartitions=10)\n",
    "    \n",
    "    # Compute the combined DataFrame\n",
    "    final_df = combined_df.compute()\n",
    "    print(final_df)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- My understanding of Normalization is based on the goals mentioned in the paper, which include the following:\n",
    "1. Remove all comments: In radare2, only ; is used for comments. Refer to 7.1 Adding Metadata for more information.\n",
    "2. Replace all numeric constant values with \"N\": Use re.sub() to iterate through all numeric constants with the regular expression \\b0x[a-fA-F0-9]+\\b|\\b\\d+\\b.\n",
    "3. Replace all irregular strings with \"M\": Use re.sub() to iterate through all irregular strings with the regular expression \\b[^a-zA-Z0-9]+\\b.\n",
    "4. Replace all function names with their short names: Use re.sub() to iterate through all common function names with the regular expression \\b(sub|loc|str|reloc|obj)_[0-9a-fA-F]+\\b.\n",
    "5. Connect the opcode and operand with \"-\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class NormalizeAssembly:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def normalize(self, inst):\n",
    "        \"\"\"\n",
    "        Normalize a given assembly instruction. According to the paper should have:\n",
    "        1. Remove all comments;\n",
    "        2. Replace all numeric constant values with \"N\";\n",
    "        3. Replace all irregular strings with \"M\";\n",
    "        4. Replace all function names with their short name;\n",
    "           For example, replace \"sub_406492\" with \"sub\", replace \"loc_100080CF\" with \"loc\".\n",
    "        5. Connect the opcode and operand with \"-\"\n",
    "        \"\"\"\n",
    "        # Remove comments\n",
    "        inst = re.sub(r';.*', '', inst)\n",
    "\n",
    "        # Replace numeric constant values with \"N\"\n",
    "        inst = re.sub(r'\\b0x[0-9a-fA-F]+\\b|\\b\\d+\\b', 'N', inst)\n",
    "\n",
    "        # Replace function names with their short name\n",
    "        inst = re.sub(r'\\b(sub|loc|str|reloc|obj)_[0-9a-fA-F]+\\b', r'\\1', inst)\n",
    "\n",
    "        # Split the instruction into parts to process irregular strings\n",
    "        asm_normed = []\n",
    "        parts = re.split(r'(\\s+)', inst)\n",
    "\n",
    "        for part in parts:\n",
    "            if re.match(r'\\b[^a-zA-Z0-9]+\\b', part):\n",
    "                part = 'M'\n",
    "            asm_normed.append(part)\n",
    "\n",
    "        # Join the parts back together\n",
    "        normalized_inst = ''.join(asm_normed)\n",
    "\n",
    "        # Connect opcode and operand with \"-\"\n",
    "        inst_parts = normalized_inst.split()\n",
    "        if len(inst_parts) > 1:\n",
    "            normalized_inst = f\"{inst_parts[0]}-{' '.join(inst_parts[1:])}\"\n",
    "        else:\n",
    "            normalized_inst = inst_parts[0]\n",
    "\n",
    "        return normalized_inst\n",
    "\n",
    "def normalize_dataframe(df, normalizer):\n",
    "    df['data'] = df['data'].apply(lambda instructions: [normalizer.normalize(inst) for inst in instructions] if isinstance(instructions, list) else instructions)\n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    normalizer = NormalizeAssembly()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My understanding is that CBOW is used for predicting a word from its context, while Skip-gram does the opposite. Thus, the inputs and outputs are reversed for these two models. In the end, a softmax function is used to predict a probability distribution.\n",
    "\n",
    "- vector_size: The dimensionality of the word vectors. The default value is 100. Generally, a larger vector size is better but requires more computational resources.\n",
    "- window: The size of the context window, which determines the number of words to consider around the target word. The default value is 5.\n",
    "- alpha: The learning rate. The default value is 0.025. The learning rate gradually decreases during training.\n",
    "- min_count: Ignores all words with a frequency lower than this value. The default value is 5.\n",
    "- sg: The training algorithm choice. 1 means Skip-gram is used, while 0 means CBOW is used. The default value is 0.\n",
    "- hs: Indicates whether hierarchical softmax is used. The default value is 0, meaning it is not used.\n",
    "- negative: The number of negative samples to use when negative sampling is employed. The default value is 5.\n",
    "- epochs: The number of training iterations. The default value is 5\n",
    "\n",
    "I spent some time on hyperparameter adjustment. After reading this paper Understanding the difficulty of training deep feedforward neural networks, I chose to use Xavier initialization for the model rather than importing the model from Gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "class Word2VecCBOW:\n",
    "    def __init__(self, vocab_size, hidden_dim):\n",
    "        # Initialize weights with Xavier initialization\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.W1 = np.random.randn(vocab_size, hidden_dim) / np.sqrt(vocab_size / 2)\n",
    "        self.W2 = np.random.randn(hidden_dim, vocab_size) / np.sqrt(hidden_dim / 2)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x))\n",
    "        return exp_x / np.sum(exp_x, axis=0)\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.h = np.dot(X, self.W1)\n",
    "        self.u = np.dot(self.h, self.W2)\n",
    "        self.y_hat = self.softmax(self.u)\n",
    "        return self.y_hat\n",
    "\n",
    "    def backward(self, X, y, learning_rate):\n",
    "        error = self.y_hat - y\n",
    "        dW2 = np.dot(self.h.T, error)\n",
    "        dW1 = np.dot(X.T, np.dot(error, self.W2.T))\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.W1 -= learning_rate * dW1\n",
    "\n",
    "    def train(self, sentences, learning_rate=0.01, epochs=10):\n",
    "        # Preprocess sentences to one-hot encoding\n",
    "        word2index = {word: i for i, word in enumerate(set(sum(sentences, [])))}\n",
    "        one_hot_encoder = OneHotEncoder(categories=[list(word2index.values())])\n",
    "        for epoch in range(epochs):\n",
    "            for sentence in sentences:\n",
    "                for i, word in enumerate(sentence):\n",
    "                    context = [word2index[w] for w in sentence[max(0, i - 2):i] + sentence[i + 1:i + 3]]\n",
    "                    target = word2index[word]\n",
    "                    X = one_hot_encoder.fit_transform([[context]]).toarray()\n",
    "                    y = one_hot_encoder.fit_transform([[target]]).toarray().flatten()\n",
    "                    self.forward(X)\n",
    "                    self.backward(X, y, learning_rate)\n",
    "\n",
    "    def get_word_vectors(self):\n",
    "        return self.W1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
